{"/":{"title":"Yash Shah","content":"# Welcome to my Personal Homepage!\nI'm always learning new things, and I'll(hopefully) share that here. My interests are broad, so you may find posts on tech, or anything else that's on my mind. My website is a work in progress, just like me :)\nI hope you enjoy it!\n\n\n# Blog Posts\n\n1. [Distributed Model Training](distributed_model_training/00_distributed_model_training.md)\n\t1. [Distributed Data Parallel](distributed_model_training/01_ddp.md)\n\t2. [Fully Sharded Data Parallel](distributed_model_training/02_fdsp.md)\n\n\n\n","lastmodified":"2023-04-21T06:40:31.78012949Z","tags":[]},"/distributed_model_training/00_distributed_model_training":{"title":"00_distributed_model_training","content":"\n# Distributed Model Training\n\nThe size of neural network models has been increasing rapidly. In order to meet these every increasing scaling needs, horizontal scaling of GPUs/TPUs is of utmost importance. There several paradigms to enable model training across multiple hardware accelerators.\n\nIn this series, I'll be mainly covering the following techniques:\n\n1. [Distributed Data Parallel](distributed_model_training/01_ddp.md)\n2. [Full Sharded Data Parallel](02_fsdp.md)\n3. [GSPMD](distributed_model_training/03_gspmd.md)\n","lastmodified":"2023-04-21T06:40:31.78012949Z","tags":[]},"/distributed_model_training/01_ddp":{"title":"01_ddp","content":"# Distributed Data Parallel","lastmodified":"2023-04-21T06:40:31.78012949Z","tags":[]},"/distributed_model_training/02_fdsp":{"title":"02_fdsp","content":"","lastmodified":"2023-04-21T06:40:31.78012949Z","tags":[]},"/distributed_model_training/03_gspmd":{"title":"03_gspmd","content":"","lastmodified":"2023-04-21T06:40:31.78012949Z","tags":[]}}