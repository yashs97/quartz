{"/":{"title":"Yash Shah","content":"# Welcome to my Personal Homepage!\n\nI'm always learning new things, and I'll(hopefully) share that here. My interests are wide-ranging, so you may find posts on tech, or anything else that's on my mind. My website is a work in progress, just like me :). I hope you enjoy your visit to this corner of the internet!\n\n\n# Blog Posts\n\n1. [Distributed Model Training](distributed_model_training/00_distributed_model_training.md)\n\t1. [Data Parallelism](distributed_model_training/01_dp.md)\n\t2. [Model Parallelism](distributed_model_training/02_mp.md)\n\t3. [Fully Sharded Data Parallel](distributed_model_training/03_fdsp.md)\n\t4. [GSPMD](distributed_model_training/04_gspmd.md)\n\n\n\n","lastmodified":"2023-04-23T02:14:56.624515291Z","tags":[]},"/distributed_model_training/00_distributed_model_training":{"title":"00_distributed_model_training","content":"\n# Distributed Model Training\n\nNeural network models are getting bigger and bigger. To keep up with this trend, it's important to be able to scale the hardware that we use to train these models. One way to do this is to use multiple accelerators, such as GPUs or TPUs. There are several different ways to train a model on multiple accelerators.\n\nIn this series, I'll be mainly covering the following techniques:\n\n1. [Data Parallelism](01_dp.md)\n2. [Model Parallelism](distributed_model_training/02_mp.md)\n3. [Full Sharded Data Parallel](distributed_model_training/02_fsdp.md)\n4. [GSPMD](04_gspmd.md)\n","lastmodified":"2023-04-23T02:14:56.624515291Z","tags":[]},"/distributed_model_training/01_dp":{"title":"01_dp","content":"# Data Parallelism\n\nData Parallelism is a technique to speed up the training process by sharding the data across multiple devices. The model is copied across several devices and each device receives a separate minibatch of data. After the gradients are computed during backpropagaion, all devices receive gradients from every other device. This global synchronisation of gradients occurs once every n iterations where `n \u003e= 1`. Every device sums all the gradients together and updates its local copy of weights. This summation is technically referred to as the `AllReduce` operation. A higher value of `n`  means that the gradients are synchronized less often which often leads to a faster training process but it can also lead to staleness of the model parameters.\n\n![image](distributed_model_training/images/ddp.png)\n\t\t\t\t\t\t\t\t\tFig 1: Data Parallel Training Process. (Image [Source](source: https://engineering.fb.com/2021/07/15/open-source/fsdp/))\n\nAt the end of the training process, the weights from each device are aggregated to create a global model.\n\nA major drawback of Data Parallelism is that it is memory ineffiecient as each device stores a copy of the entire model.","lastmodified":"2023-04-23T02:14:56.624515291Z","tags":[]},"/distributed_model_training/02_mp":{"title":"02_mp","content":"","lastmodified":"2023-04-23T02:14:56.624515291Z","tags":[]},"/distributed_model_training/03_fdsp":{"title":"03_fdsp","content":"","lastmodified":"2023-04-23T02:14:56.624515291Z","tags":[]},"/distributed_model_training/04_gspmd":{"title":"04_gspmd","content":"","lastmodified":"2023-04-23T02:14:56.624515291Z","tags":[]}}