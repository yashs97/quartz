{"/":{"title":"Yash Shah","content":"# Welcome to my Personal Homepage!\n\nI'm always learning new things, and I'll(hopefully) share that here. My interests are wide-ranging, so you may find posts on tech, or anything else that's on my mind. My website is a work in progress, just like me :). I hope you enjoy your visit to this corner of the internet!\n\n\n# Blog Posts\n\n* Distributed Model Training\n\t- [Data and Model Parallelism](distributed_model_training/01_dp_and_mp.md)\n\t- [Fully Sharded Data Parallel](distributed_model_training/02_fdsp.md)\n\t- [GSPMD](distributed_model_training/03_gspmd.md)\n\n\n\n","lastmodified":"2023-04-24T01:58:40.00653783Z","tags":[]},"/distributed_model_training/01_dp_and_mp":{"title":"Distributed Model Training","content":"\nNeural network models are getting bigger and bigger. To keep up with this trend, it's important to be able to scale the hardware that we use to train these models. One way to do this is to use multiple accelerators, such as GPUs or TPUs. There are several different ways to train a model on multiple accelerators. Data parallelism and model parallelism are the two main categories of parallelism techniques, and each specialized technique falls under one of these categories.\n\n\u003e This post is intended to provide a basic understanding of data and model parallelism. For a more in-depth treatment of these topics, please refer to Lillian Weng's excellent [article](https://lilianweng.github.io/posts/2021-09-25-train-large/).\n\n## Data Parallelism\n\nData Parallelism is a technique to speed up the training process by sharding the data across multiple devices. The model is copied across several devices and each device receives a separate mini-batch of data. After the gradients are computed during backpropagation, all devices receive gradients from every other device. This global synchronization of gradients occurs once every n iterations where `n \u003e= 1`. Every device sums all the gradients together and updates its local copy of weights. This summation is technically referred to as the `AllReduce` operation. A higher value of `n`  means that the gradients are synchronized less often which often leads to a faster training process but it can also lead to staleness of the model parameters.\t\t\t\t\t\t\t\t\t\t\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"images/ddp.png\" alt=\"Fig 1: Data Parallel Training Process. (Image [Source](https://engineering.fb.com/2021/07/15/open-source/fsdp/))\" /\u003e\n\u003c/p\u003e\n\nAt the end of the training process, the weights from each device are aggregated to create a global model.\nA major drawback of Data Parallelism is that it is memory inefficient as each device stores a copy of the entire model.\n\n## Model Parallelism \n\nModel parallelism is a technique used to train machine learning models that are too large to fit on a single device. Model parameters are sharded across multiple devices and each device is responsible for the computation of that particular shard. Compared to Data Parallelism, memory footprint and computation workload of each device is lower. However, there is a significant increase in communication overhead as activations need to be communicated across devices. \n\n### Pipeline Parallelism\n\nNeural Networks consists of layers built on top of another which makes their sharding process trivial. However, if a system of these devices trains a network in a sequential manner it can lead to long idle times for some devices. Pipeline Parallelism can significantly reduces per device idle time and it follows the same principles as [instruction pipelining](https://en.wikipedia.org/wiki/Instruction_pipelining) in a single processor.\nEach device stores a layer (or more) and processes a different micro-batch of data in parallel. Every micro-batch of data undergoes both the forward and backward passes through each device. The number of workers that the model is split over is commonly known as pipeline depth.\n\nGradients can be aggregated either in a synchronous or an asynchronous manner at the end.\nIn [GPipe](https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html), gradients are aggregated and applied synchronously for each micro-batch of data at the end.\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"images/gpipe.png\" alt=\"Fig 2: Naive MP vs GPipe (Image [source](https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html))\" /\u003e\n\u003c/p\u003e\n\n[PipeDream](https://www.pdl.cmu.edu/PDL-FTP/BigLearning/sosp19-final271.pdf) is an example of asynchronous gradient accumulation and aims to reduce the bubble time highlighted in the above image. It does so by alternating between the forward and backward passes for each worker after an initial phase called startup time. PipeDream uses a technique called weight stashing where each device tracks several model versions to make sure that the same version of model weights are used for each forward and backward pass. [PipeDream-2BW](https://arxiv.org/pdf/2006.09503.pdf) further saves memory by stashing only two versions of a model.\n\n\n[Tensor Parallelism](https://lilianweng.github.io/posts/2021-09-25-train-large/#tensor-parallelism) is a another method of model parallelism where instead of partitioning a model vertically, a tensor (e.g., input image or model parameters) is sharded across multiple devices. [Sequence Parallelism](https://arxiv.org/pdf/2105.13120.pdf) is another strategy that has been developed to address long sequence length limitations for LLMs and train them efficiently.","lastmodified":"2023-04-24T01:58:40.00653783Z","tags":["deep_learning"]},"/distributed_model_training/02_fdsp":{"title":"Fully Sharded Data Parallel","content":"\nFully Sharded Data Parallel (FSDP) is a distributed training algorithm that partitions both the dataset and model parameters across devices. FSDP can be classified as a data parallel algorithm since each device performs the computation on its local subset of the training data for each micro-batch\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"images/fsdp.png\" alt=\"Fig 1: FSDP Workflow. (Image [Source](https://engineering.fb.com/2021/07/15/open-source/fsdp/))\" /\u003e\n\u003c/p\u003e\n\nThe workflow of FSDP is as follows:\n1. Each device is responsible for a shard of the model parameters and receives a micro-batch of data for training.\n2. Before the forward pass, FSDP performs an `All-Gather` operation on each device and releases the model weights that are not part of its assigned shard. This allows each device to store only the weights it needs, resulting in improved performance.\n3. Another `All-Gather` operation is performed before the backward pass.\n4. After backpropagation, each device has access to the gradients for all parameters for its micro-batch of data. Each device then receives gradients from other devices for its assigned shard of model parameters and discards gradients that are not part of its shard. This process is known as the `Reduce-Scatter` operation.\n5. Local weight updates are performed next.\n\nCompared to traditional Model Parallelism, FSDP has a reduced communication overhead since there is no need to communicate intermediate activations between devices. However, when dealing with very large models or datasets, there is still a considerable amount of communication overhead that needs to be managed. This is due to the need to synchronize gradients across devices during the `All-Gather` and `Reduce-Scatter` operations.\n\n\n\n\n\n\n\n","lastmodified":"2023-04-24T01:58:40.00653783Z","tags":["deep_learning"]},"/distributed_model_training/03_gspmd":{"title":"03_gspmd","content":"","lastmodified":"2023-04-24T01:58:40.00653783Z","tags":[]}}