<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="This post is intended to provide a basic understanding of data and model parallelism. For a more in-depth treatment of these topics, please refer to Lillian Weng&rsquo;s excellent article."><meta property="og:title" content><meta property="og:description" content="This post is intended to provide a basic understanding of data and model parallelism. For a more in-depth treatment of these topics, please refer to Lillian Weng&rsquo;s excellent article."><meta property="og:type" content="website"><meta property="og:image" content="https://yashs97.github.io/quartz/icon.png"><meta property="og:url" content="https://yashs97.github.io/quartz/distributed_model_training/01_dp_and_mp/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="This post is intended to provide a basic understanding of data and model parallelism. For a more in-depth treatment of these topics, please refer to Lillian Weng&rsquo;s excellent article."><meta name=twitter:image content="https://yashs97.github.io/quartz/icon.png"><title>Yash Shah</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://yashs97.github.io/quartz//icon.png><link href=https://yashs97.github.io/quartz/styles.80333fa2099c0bee674efa435fde378c.min.css rel=stylesheet><link href=https://yashs97.github.io/quartz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://yashs97.github.io/quartz/js/darkmode.1473e0cff4d3b8a595f352bbb8877f15.min.js></script>
<script src=https://yashs97.github.io/quartz/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://yashs97.github.io/quartz/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://yashs97.github.io/quartz/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://yashs97.github.io/quartz/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://yashs97.github.io/quartz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://yashs97.github.io/quartz/",fetchData=Promise.all([fetch("https://yashs97.github.io/quartz/indices/linkIndex.ecb0791eed6205f3ef6499c5f7cf5475.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://yashs97.github.io/quartz/indices/contentIndex.6ded29f70d1bafd33663d663a2e63563.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://yashs97.github.io/quartz",!1);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://yashs97.github.io/quartz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/yashs97.github.io\/quartz\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=yashs97.github.io/quartz src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://yashs97.github.io/quartz/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://yashs97.github.io/quartz/>Yash Shah</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Last updated
Apr 23, 2023
<a href=https://github.com/yashs97/quartz/tree/hugo/content/distributed_model_training/01_dp_and_mp.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#pipeline-parallelism>Pipeline Parallelism</a></li></ol></nav></details></aside><blockquote><p>This post is intended to provide a basic understanding of data and model parallelism. For a more in-depth treatment of these topics, please refer to Lillian Weng&rsquo;s excellent
<a href=https://lilianweng.github.io/posts/2021-09-25-train-large/ rel=noopener>article</a>.</p></blockquote><a href=#data-parallelism><h1 id=data-parallelism><span class=hanchor arialabel=Anchor># </span>Data Parallelism</h1></a><p>Data Parallelism is a technique to speed up the training process by sharding the data across multiple devices. The model is copied across several devices and each device receives a separate mini-batch of data. After the gradients are computed during backpropagation, all devices receive gradients from every other device. This global synchronization of gradients occurs once every n iterations where <code>n >= 1</code>. Every device sums all the gradients together and updates its local copy of weights. This summation is technically referred to as the <code>AllReduce</code> operation. A higher value of <code>n</code> means that the gradients are synchronized less often which often leads to a faster training process but it can also lead to staleness of the model parameters.</p><p><img src=https://yashs97.github.io/quartz//distributed_model_training/images/ddp.png width=auto alt=image>
Fig 1: Data Parallel Training Process. (Image [Source](source:
<a href=https://engineering.fb.com/2021/07/15/open-source/fsdp/ rel=noopener>https://engineering.fb.com/2021/07/15/open-source/fsdp/</a>))</p><p>At the end of the training process, the weights from each device are aggregated to create a global model.</p><p>A major drawback of Data Parallelism is that it is memory ineffiecient as each device stores a copy of the entire model.</p><a href=#model-parallelism><h1 id=model-parallelism><span class=hanchor arialabel=Anchor># </span>Model Parallelism</h1></a><p>Model parallelism is a technique used to train machine learning models that are too large to fit on a single device. Model parameters are sharded across multiple devices and each device is responsible for the computation of that particular shard. Compared to Data Parallelism, memory footprint and computation workload of each device is lower. However, there is a significant increase in communication overhead as activations need to be communicated across devices.</p><a href=#pipeline-parallelism><h2 id=pipeline-parallelism><span class=hanchor arialabel=Anchor># </span>Pipeline Parallelism</h2></a><p>Neural Networks consists of layers built on top of another which makes their sharding process trivial. However, if a system of these devices trains a network in a sequential manner it can lead to long idle times for some devices. Pipeline Parallelism can significantly reduces per device idle time and it follows the same principles as
<a href=https://en.wikipedia.org/wiki/Instruction_pipelining rel=noopener>instruction pipelining</a> in a single processor.
Each device stores a layer (or more) and processes a different micro-batch of data in parallel. Every micro-batch of data undergoes both the forward and backward passes through each device.</p><p>Gradients can be aggregated either in a synchronous or an asynchronous manner at the end.
In
<a href=https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html rel=noopener>GPipe</a>, gradients are aggregated and applied synchronously for each micro-batch of data at the end.
<img src=https://yashs97.github.io/quartz//images/gpipe.png width=auto alt>
Fig 1: Naive MP vs GPipe (Image
<a href=https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html rel=noopener>source</a>)</p><p><a href=https://www.pdl.cmu.edu/PDL-FTP/BigLearning/sosp19-final271.pdf rel=noopener>PipeDream</a> is an example of asynchronous gradient accumulation and aims to reduce the bubble time highlighted in the above image. It does so by alternating between the forward and backward passes for each worker after an initial phase called startup time. PipeDream uses a technique called weight stashing where each device tracks several model versions to make sure that the same version of model weights are used for each forward and backward pass.
<a href=https://arxiv.org/pdf/2006.09503.pdf rel=noopener>PipeDream-2BW</a> further saves memory by stashing only two versions of a model.</p><p><a href=https://lilianweng.github.io/posts/2021-09-25-train-large/#tensor-parallelism rel=noopener>Tensor Parallelism</a> is a another method of model parallelism where instead of partitioning a model vertically, a tensor (e.g., input image or model parameters) is sharded across multiple devices.
<a href=https://arxiv.org/pdf/2105.13120.pdf rel=noopener>Sequence Parallelism</a> is another strategy that has been developed to address long sequence length limitations for LLMs and train them efficiently.</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/quartz// data-ctx="Data and Model Parallelism" data-src=/ class=internal-link>Yash Shah</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://yashs97.github.io/quartz/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Yash Shah using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2023</p><ul><li><a href=https://yashs97.github.io/quartz/>Home</a></li><li><a href=https://github.com/yashs97>GitHub</a></li></ul></footer></div></div></body></html>