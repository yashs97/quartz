# Data Parallelism

Data Parallelism is a technique to speed up the training process by sharding the data across multiple devices. The model is copied across several devices and each device receives a separate minibatch of data. After the gradients are computed during backpropagaion, all devices receive gradients from every other device. This global synchronisation of gradients occurs once every n iterations where `n >= 1`. Every device sums all the gradients together and updates its local copy of weights. This summation is technically referred to as the `AllReduce` operation. A higher value of `n`  means that the gradients are synchronized less often which often leads to a faster training process but it can also lead to staleness of the model parameters.

![image](distributed_model_training/images/ddp.png)
									Fig 1: Data Parallel Training Process. (Image [Source](source: https://engineering.fb.com/2021/07/15/open-source/fsdp/))

At the end of the training process, the weights from each device are aggregated to create a global model.

A major drawback of Data Parallelism is that it is memory ineffiecient as each device stores a copy of the entire model.